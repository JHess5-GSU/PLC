{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['<jason>', 'JASON'], ['{', 'LCURLY'], ['yar', 'CHAR'], ['smolboy', 'VARNAME'], [';', 'SC'], ['ent', 'INT'], ['alpha_', 'VARNAME'], [';', 'SC'], ['shart', 'SHORT'], ['bravo_', 'VARNAME'], [';', 'SC'], ['smolboy', 'VARNAME'], ['=', 'EQUALS'], ['10', 'NUM'], [';', 'SC'], ['smolboy', 'VARNAME'], ['=', 'EQUALS'], ['smolboy', 'VARNAME'], [';', 'SC'], ['alpha_', 'VARNAME'], ['=', 'EQUALS'], ['smolboy', 'VARNAME'], [';', 'SC'], ['smolboy', 'VARNAME'], ['=', 'EQUALS'], ['20', 'NUM'], [';', 'SC'], ['bravo_', 'VARNAME'], ['=', 'EQUALS'], ['alpha_', 'VARNAME'], [';', 'SC'], ['}', 'RCURLY'], ['<nosaj>', 'NOSAJ']]\n",
      "\n",
      "\n",
      "Start found\n",
      "New variable smolboy of type yar\n",
      "New variable alpha_ of type ent\n",
      "New variable bravo_ of type shart\n",
      "Assigned smolboy a value of 10\n",
      "Assigned smolboy the value of smolboy, which was 10\n",
      "Assigned alpha_ the value of smolboy, which was 10\n",
      "Assigned smolboy a value of 20\n",
      "Assigned bravo_ the value of alpha_, which was 10\n",
      "Complete\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re               # Thank God for regular expressions\n",
    "import sys\n",
    "\n",
    "class Tokenize():\n",
    "    def __init__(self, infile):\n",
    "        self.my_code = open(infile, 'r')\n",
    "\n",
    "    def run(self, outfile):\n",
    "\n",
    "        ERROR_FUSE = False      # Blow this fuse on errors so we know, but keep going.\n",
    "        ERROR_LINE = 0          \n",
    "\n",
    "        lex_tok_list = []       # Stores my lex's and my tok's.   [[\"Lex\",\"Token\"],...]\n",
    "\n",
    "        var_regex = re.compile(\"^(([A-Za-z]*[_]*)+)$\")\n",
    "        # Match any combo of letters and underscores. Will check length first below.\n",
    "\n",
    "        num_regex = re.compile(\"^(\\d+)$\")\n",
    "        # Match any amount of digits.\n",
    "\n",
    "        comm_regex = re.compile(\"^(#+)$\")\n",
    "\n",
    "        for line in self.my_code:    # Go through all lines of source code\n",
    "\n",
    "            ERROR_LINE += 1     # Today I learned that line counts start at 1, not 0.   \n",
    "            i = 0\n",
    "            line = line.replace(\"\\n\",\" \\\\n\")    # To help me detect newlines.\n",
    "            string = line.split(' ')        # In this lang, lex's are sep'd by spaces\n",
    "            last_index = len(string) - 1\n",
    "            last_str = string[last_index]\n",
    "            last_str_len = len(last_str)\n",
    "\n",
    "            while (i+1 <= len(string)):     \n",
    "                ERROR_FLAG = False\n",
    "\n",
    "                if (string[i] == '\\\\n'):\n",
    "                    pass\n",
    "                    #lex_tok_list.append([\"<NL>\",\"NEWL\"])       # Ignoring newlines.\n",
    "\n",
    "                elif (string[i] == '<jason>'):\n",
    "                    lex_tok_list.append([string[i],\"JASON\"])\n",
    "                elif (string[i] == '<nosaj>'):\n",
    "                    lex_tok_list.append([string[i],\"NOSAJ\"])\n",
    "\n",
    "\n",
    "                elif (string[i] == ';'):\n",
    "                    lex_tok_list.append([string[i],\"SC\"])\n",
    "\n",
    "                elif (string[i] == '+'):\n",
    "                    lex_tok_list.append([string[i],\"PLUS\"])\n",
    "                elif (string[i] == '++'):\n",
    "                    lex_tok_list.append([string[i],\"INC\"])\n",
    "\n",
    "                elif (string[i] == '-'):\n",
    "                    lex_tok_list.append([string[i],\"MINUS\"])\n",
    "                elif (string[i] == '--'):\n",
    "                    lex_tok_list.append([string[i],\"DEC\"])\n",
    "\n",
    "                elif (string[i] == '*'):\n",
    "                    lex_tok_list.append([string[i],\"MULT\"])\n",
    "                elif (string[i] == '/'):\n",
    "                    lex_tok_list.append([string[i],\"DIV\"])\n",
    "                elif (string[i] == '%'):\n",
    "                    lex_tok_list.append([string[i],\"MOD\"])\n",
    "\n",
    "                elif (string[i] == '<'):\n",
    "                    lex_tok_list.append([string[i],\"LT\"])\n",
    "                elif (string[i] == '>'):\n",
    "                    lex_tok_list.append([string[i],\"GT\"])\n",
    "                elif (string[i] == '<='):\n",
    "                    lex_tok_list.append([string[i],\"LTE\"])\n",
    "                elif (string[i] == '>='):\n",
    "                    lex_tok_list.append([string[i],\"GTE\"])\n",
    "\n",
    "                elif (string[i] == '='):\n",
    "                    lex_tok_list.append([string[i],\"EQUALS\"])\n",
    "                elif (string[i] == '=='):\n",
    "                    lex_tok_list.append([string[i],\"EQCOMP\"])\n",
    "                elif (string[i] == '!='):\n",
    "                    lex_tok_list.append([string[i],\"NEQCOMP\"])\n",
    "\n",
    "                elif (string[i] == '&&'):\n",
    "                    lex_tok_list.append([string[i],\"BAND\"])\n",
    "                elif (string[i] == '||'):\n",
    "                    lex_tok_list.append([string[i],\"BOR\"])                    \n",
    "        \n",
    "                elif (string[i] == '('):\n",
    "                    lex_tok_list.append([string[i],\"LP\"])                \n",
    "                elif (string[i] == ')'):\n",
    "                    lex_tok_list.append([string[i],\"RP\"])\n",
    "\n",
    "                elif (string[i] == 'print'):\n",
    "                    lex_tok_list.append([string[i],\"PRINT\"])\n",
    "\n",
    "                elif (string[i] == 'ef'):\n",
    "                    lex_tok_list.append([string[i],\"EF\"])                \n",
    "                elif (string[i] == 'four'):\n",
    "                    lex_tok_list.append([string[i],\"FOUR\"])\n",
    "                elif (string[i] == 'yl'):\n",
    "                    lex_tok_list.append([string[i],\"YL\"])\n",
    "                elif (string[i] == 'ls'):\n",
    "                    lex_tok_list.append([string[i],\"LS\"])\n",
    "                elif (string[i] == 'dew'):\n",
    "                    lex_tok_list.append([string[i],\"DEW\"])\n",
    "\n",
    "                elif (string[i] == '{'):\n",
    "                    lex_tok_list.append([string[i],\"LCURLY\"])\n",
    "                elif (string[i] == '}'):\n",
    "                    lex_tok_list.append([string[i],\"RCURLY\"])\n",
    "\n",
    "                elif (string[i] == 'yar'):\n",
    "                    lex_tok_list.append([string[i],\"CHAR\"])                \n",
    "                elif (string[i] == 'ent'):\n",
    "                    lex_tok_list.append([string[i],\"INT\"])\n",
    "                elif (string[i] == 'shart'):\n",
    "                    lex_tok_list.append([string[i],\"SHORT\"])\n",
    "                elif (string[i] == 'lawn'):\n",
    "                    lex_tok_list.append([string[i],\"LONG\"])\n",
    "\n",
    "                elif (num_regex.match(string[i])):\n",
    "                    lex_tok_list.append([string[i],\"NUM\"])\n",
    "\n",
    "                elif (len(string[i]) >= 6 and len(string[i]) <= 8):\n",
    "                    if (var_regex.match(string[i]) != None):\n",
    "                        lex_tok_list.append([string[i],\"VARNAME\"])\n",
    "                    else:\n",
    "                        #lex_tok_list.append([string[i],\"BAD_VAR\"])\n",
    "                        ERROR_FLAG = True\n",
    "\n",
    "                elif (string[i] == ''):\n",
    "                    pass\n",
    "\n",
    "                elif (comm_regex.match(string[i])):\n",
    "                    #lex_tok_list.append([string[i],\"COMMJASON\"])   # Ignore comments\n",
    "                    i+=1\n",
    "                    \n",
    "                    while (i+1 <= len(string)):\n",
    "                        if (string[i] != \"\\\\n\"):\n",
    "                            #lex_tok_list.append([string[i],\"COMMTEXT\"])   # Ignore comments\n",
    "                            pass\n",
    "                        else:\n",
    "                            #lex_tok_list.append([\"<NL>\",\"NEWL\"])       \n",
    "                            pass\n",
    "                            break\n",
    "                        i+=1            \n",
    "\n",
    "                elif (string[i] == ','):\n",
    "                    lex_tok_list.append([\",\",\"COMMA\"])\n",
    "\n",
    "                else:\n",
    "                    lex_tok_list.append([string[i],\"UNKNOWN\"])\n",
    "                    ERROR_FLAG = True\n",
    "\n",
    "                if (ERROR_FLAG):\n",
    "                    print(\"THERE WAS AN ERROR ON LINE {} - BAD LEXEME WAS: {}\" \\\n",
    "                        .format(ERROR_LINE, string[i]))\n",
    "                    ERROR_FUSE = True\n",
    "\n",
    "                i+=1\n",
    "\n",
    "        if (ERROR_FUSE):\n",
    "            print(\"No output due to error(s)\")\n",
    "        else:\n",
    "            self.my_tokens = open(outfile, 'w')\n",
    "            for x in range(len(lex_tok_list)):\n",
    "                #print(lex_tok_list[x])\n",
    "                self.my_tokens.write(str(lex_tok_list[x])+'\\n')\n",
    "                \n",
    "            self.my_tokens.close()\n",
    "            print(lex_tok_list)\n",
    "\n",
    "        self.my_code.close()\n",
    "        \n",
    "\n",
    "        print()\n",
    "        print()\n",
    "\n",
    "\n",
    "\n",
    "class ParseTokens():\n",
    "\n",
    "    my_vars = {}\n",
    "    var_types = {}\n",
    "\n",
    "    def __init__(self, infile):\n",
    "\n",
    "        self.index = 0\n",
    "        self.my_tokens = open(infile, 'r')\n",
    "        self.lines = self.my_tokens.readlines()\n",
    "\n",
    "        for x in range(len(self.lines)):\n",
    "            self.lines[x] = self.lines[x].strip('\\'').strip('][\\n').split(', ')\n",
    "            \n",
    "            for y in range(2):\n",
    "                self.lines[x][y] = self.lines[x][y].strip('\\'')\n",
    "\n",
    "            #print(self.lines[x])\n",
    "\n",
    "        self.current_token = self.lines[0][1]\n",
    "        self.current_lexeme = self.lines[0][0]\n",
    "        self.current_element = self.lines[0]\n",
    "        \n",
    "\n",
    "    def get_next_token(self):\n",
    "        if (self.index + 1 >= len(self.lines)):\n",
    "            return None\n",
    "\n",
    "\n",
    "        self.index += 1\n",
    "        self.current_element = self.lines[self.index]\n",
    "        self.current_lexeme = self.lines[self.index][0]\n",
    "        self.current_token = self.lines[self.index][1]\n",
    "\n",
    "\n",
    "        return self.current_token\n",
    "        \n",
    "\n",
    "# RULES OF PRODUCTION RULES: \n",
    "    # Curly brackets '{' or '}' mean ONE OR MORE REPETITIONS\n",
    "    # Square brackets '[' or ']' mean ZERO OR MORE REPETITIONS\n",
    "    # '\\' is an escape character, so take the next symbol literally.\n",
    "    # '|' serves to mean 'OR', allowing alternative options.\n",
    "    # THANK YOU   <3\n",
    "\n",
    "# PRODUCTION RULES:\n",
    "    # <program> :  JASON <statement_list> NOSAJ\n",
    "\n",
    "    # <statement_list> :  \\{ { <statement> ; } \\}\n",
    "    # <statement> :  <declaration> | <if_statement> | <while_statement> | <ass>\n",
    "\n",
    "    # <declaration> :  <id> VARNAME\n",
    "    # <id> :  CHAR | INT | SHORT | LONG\n",
    "\n",
    "    # <expression> :  <term> { (MULT | DIV | MOD) <term> }                      # Mult, div, modulo, are lower precedence\n",
    "    # <term> :  <factor> { (ADD | SUB) <factor> }                               # Add and subtraction are higher precedence\n",
    "\n",
    "    # <bool_expression> :  <band> [ OR <band> ]\n",
    "    # <band> :  <beq> [ AND <beq> ]\n",
    "    # <beq> :  <brel> { ( DNEQ | EQ ) <brel> } \n",
    "    # <brel> :  <expr> \n",
    "\n",
    "    # <if_statement> :  EF \\( <bool_expression> \\) [ LS <statement> ]\n",
    "\n",
    "    # <while_statement> :  YL \\( <bool_expression> \\) <statement_list>\n",
    "\n",
    "    # <ass> :  VARNAME = <factor>\n",
    "\n",
    "    # <factor> :  CHAR | INT | SHORT | LONG | NUMBER | \\( <expression> \\)       # Expressions in parentheses take highest precedence\n",
    "\n",
    "\n",
    "\n",
    "    def program(self):\n",
    "        if self.current_token == 'JASON':\n",
    "            print(\"Start found\")\n",
    "            self.get_next_token()\n",
    "            self.statement_list()\n",
    "\n",
    "            if self.get_next_token() != 'NOSAJ':\n",
    "                print(\"Missing end of program.\")\n",
    "                sys.exit()\n",
    "            else:\n",
    "                print(\"Complete\")\n",
    "                print()\n",
    "\n",
    "    def statement_list(self):\n",
    "        if self.current_token == 'LCURLY':\n",
    "            #print(\"Statement start found\")\n",
    "            self.get_next_token()\n",
    "\n",
    "            while (self.current_token == 'EF' or self.current_token == 'YL' or self.current_token == 'CHAR' or self.current_token == 'INT'\\\n",
    "                 or self.current_token == 'SHORT' or self.current_token == 'LONG' or self.current_token == 'LCURLY' or self.current_token == 'VARNAME'):\n",
    "                self.statement()\n",
    "\n",
    "                if (self.get_next_token() != \"SC\"):\n",
    "                    self.error()            \n",
    "                #print(\"SC Found\")\n",
    "                self.get_next_token()\n",
    "\n",
    "            if (self.current_token != 'RCURLY'):\n",
    "                print(\"{} Found instead of RCURLY\".format(self.current_element))\n",
    "                self.error()\n",
    "\n",
    "    def statement(self):\n",
    "        if (self.current_token == 'EF'):\n",
    "            self.if_statement()\n",
    "        elif (self.current_token == 'YL'):\n",
    "            self.while_statement()\n",
    "        elif (self.current_token == 'CHAR'):\n",
    "            self.declare()\n",
    "        elif (self.current_token == 'INT'):\n",
    "            self.declare()\n",
    "        elif (self.current_token == 'SHORT'):\n",
    "            self.declare()\n",
    "        elif (self.current_token == 'LONG'):\n",
    "            self.declare()\n",
    "        elif (self.current_token == 'VARNAME'):\n",
    "            self.ass()\n",
    "        elif (self.current_token == 'LCURLY'):\n",
    "            self.statement_list()\n",
    "\n",
    "    def while_statement(self):\n",
    "        pass\n",
    "\n",
    "    def if_statement(self):\n",
    "        self.get_next_token()\n",
    "\n",
    "    def var(self):\n",
    "        pass\n",
    "\n",
    "    def declare(self):\n",
    "        if (self.get_next_token() != 'VARNAME'):\n",
    "            self.error()\n",
    "        else:\n",
    "            #self.lines[self.index][2] = 0\n",
    "            self.my_vars[self.current_lexeme] = None\n",
    "            #print(self.my_vars)\n",
    "            self.var_types[self.current_lexeme] = self.lines[self.index-1][0]\n",
    "            print(\"New variable {} of type {}\".format(self.current_lexeme, self.var_types[self.current_lexeme]))\n",
    "\n",
    "    def ass(self):\n",
    "        # <ass> :  VARNAME = <factor>\n",
    "        if self.current_token == 'VARNAME':\n",
    "            if (self.get_next_token() == 'EQUALS'):\n",
    "                self.factor()\n",
    "            else:\n",
    "                self.error()\n",
    "        else:\n",
    "            self.error()\n",
    "    \n",
    "    def factor(self):\n",
    "        #print(\"Factor Found\")\n",
    "        self.get_next_token()\n",
    "\n",
    "        if (self.current_token == 'VARNAME'):\n",
    "            self.my_vars[self.lines[self.index-2][0]] = self.my_vars[self.current_lexeme]\n",
    "            print(\"Assigned {} the value of {}, which was {}\".format(self.lines[self.index-2][0], self.current_lexeme, self.my_vars[self.lines[self.index-2][0]]))\n",
    "\n",
    "        elif (self.current_token == 'NUM'):\n",
    "            self.my_vars[self.lines[self.index-2][0]] = self.lines[self.index][0]\n",
    "            print(\"Assigned {} a value of {}\".format(self.lines[self.index-2][0], self.lines[self.index][0]))\n",
    "            #print(self.my_vars[self.lines[self.index-2][0]])\n",
    "\n",
    "        elif (self.current_token == 'LPAREN'):\n",
    "            self.express()\n",
    "            if (self.get_next_token() != 'RPAREN'):\n",
    "                self.error()\n",
    "        else:\n",
    "            self.error()\n",
    "\n",
    "\n",
    "    def express(self):\n",
    "        self.term()\n",
    "        while (self.current_token == 'MULT' or self.current_token == 'DIV' or self.current_token == 'MOD'):\n",
    "            self.get_next_token()\n",
    "            self.term()     \n",
    "\n",
    "    def term(self):\n",
    "        self.factor()\n",
    "        while (self.current_token == 'ADD' or self.current_token == 'SUB'):\n",
    "            self.get_next_token()\n",
    "            self.term()\n",
    "\n",
    "    def error(self):\n",
    "        print(\"There was an error at {}, line {}. Stopping\".format(self.current_element, self.index+1))\n",
    "        sys.exit()\n",
    "        \n",
    "    def run(self):\n",
    "        self.program()\n",
    "\n",
    "        self.my_tokens.close()\n",
    "\n",
    "#Tokenize(\"testcode.txt\", \"testtokens.txt\").run()\n",
    "#ParseTokens(\"testtokens.txt\").run()\n",
    "#ParseTokens(\"tokens.txt\").run()\n",
    "\n",
    "\n",
    "Tokenize(\"testcode1.txt\").run('testtokens1.txt')\n",
    "ParseTokens(\"testtokens1.txt\").run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['<jason>', 'JASON'], ['{', 'LCURLY'], ['yar', 'CHAR'], ['smolboy', 'VARNAME'], [';', 'SC'], ['smolboy', 'VARNAME'], ['=', 'EQUALS'], ['0', 'NUM'], [';', 'SC'], ['shart', 'SHORT'], ['alpha_', 'VARNAME'], [';', 'SC'], ['alpha_', 'VARNAME'], ['=', 'EQUALS'], ['3', 'NUM'], [';', 'SC'], ['ent', 'INT'], ['bravo_', 'VARNAME'], [';', 'SC'], ['bravo_', 'VARNAME'], ['=', 'EQUALS'], ['8008', 'NUM'], [';', 'SC'], ['lawn', 'LONG'], ['charlie_', 'VARNAME'], [';', 'SC'], ['charlie_', 'VARNAME'], ['=', 'EQUALS'], ['1234567890', 'NUM'], [';', 'SC'], ['}', 'RCURLY'], ['<nosaj>', 'NOSAJ']]\n",
      "\n",
      "\n",
      "Start found\n",
      "New variable smolboy of type yar\n",
      "Assigned smolboy a value of 0\n",
      "New variable alpha_ of type shart\n",
      "Assigned alpha_ a value of 3\n",
      "New variable bravo_ of type ent\n",
      "Assigned bravo_ a value of 8008\n",
      "New variable charlie_ of type lawn\n",
      "Assigned charlie_ a value of 1234567890\n",
      "Complete\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Tokenize(\"testcode2.txt\").run('testtokens2.txt')\n",
    "ParseTokens(\"testtokens2.txt\").run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THERE WAS AN ERROR ON LINE 4 - BAD LEXEME WAS: smolboy1\n",
      "THERE WAS AN ERROR ON LINE 7 - BAD LEXEME WAS: alphaaaa_\n",
      "THERE WAS AN ERROR ON LINE 10 - BAD LEXEME WAS: ;;\n",
      "THERE WAS AN ERROR ON LINE 14 - BAD LEXEME WAS: ===\n",
      "No output due to error(s)\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [117], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m Tokenize(\u001b[39m\"\u001b[39m\u001b[39mtestcode3.txt\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mrun(\u001b[39m'\u001b[39m\u001b[39mtesttokens3.txt\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m ParseTokens(\u001b[39m\"\u001b[39;49m\u001b[39mtesttokens3.txt\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39mrun()\n",
      "Cell \u001b[1;32mIn [113], line 199\u001b[0m, in \u001b[0;36mParseTokens.__init__\u001b[1;34m(self, infile)\u001b[0m\n\u001b[0;32m    195\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlines[x][y] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlines[x][y]\u001b[39m.\u001b[39mstrip(\u001b[39m'\u001b[39m\u001b[39m\\'\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m    197\u001b[0m     \u001b[39m#print(self.lines[x])\u001b[39;00m\n\u001b[1;32m--> 199\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_token \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlines[\u001b[39m0\u001b[39;49m][\u001b[39m1\u001b[39m]\n\u001b[0;32m    200\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_lexeme \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlines[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m]\n\u001b[0;32m    201\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_element \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlines[\u001b[39m0\u001b[39m]\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "Tokenize(\"testcode3.txt\").run('testtokens3.txt')\n",
    "ParseTokens(\"testtokens3.txt\").run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tokenize(\"testcode4.txt\").run('testtokens4.txt')\n",
    "ParseTokens(\"testtokens4.txt\").run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "99d5832e79bb9ad7eec712a131b25deca841e5a96b57bc478d23e5bca9e46809"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
